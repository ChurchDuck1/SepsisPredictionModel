{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSC 296S Deep Learning (Spring 2026)\n",
    "\n",
    "#### Dr. Haiquan Chen, Dept of Computer Scicence\n",
    "\n",
    "#### California State University, Sacramento\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inport statements\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#testing variables, set True for more in depth results\n",
    "testing = False\n",
    "testingResults = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#modify the data\\npreparedPatients = []\\nfor i in range (1,6001):\\n    temp = patients[f\"p{i:06d}\"]\\n    #drop unneccessary fields\\n    temp.drop([\\'Bilirubin_direct\\', \\'Bilirubin_total\\', \\'TroponinI\\', \\'Fibrinogen\\', \\'Unit1\\', \\'Unit2\\', \\'EtCO2\\'], axis=1, inplace=True)\\n    row = temp.mean() #using the average per patient if applicatble\\n    row = row.fillna(global_means) #if average can\\'t be calculated per patient, use global average\\n    row[\\'SepsisLabel\\'] = temp[\\'SepsisLabel\\'].iloc[-1] #overwrite sepsis label with last label \\n\\n    preparedPatients.append(row)\\n\\n#store as a dataframe\\ndf_patients = pd.DataFrame(preparedPatients)\\n\\n#xy split (not using to_xy)\\ny = df_patients[\\'SepsisLabel\\'].values.astype(np.float32) \\nx = df_patients.drop(\\'SepsisLabel\\', axis=1).values.astype(np.float32)\\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is a function that returns x as an average for each field for x and the final sepsis value as y.  However, I believe window size is better\n",
    "#EX:\n",
    "# 0 0 0 | 0\n",
    "# 0 1 2 | 1\n",
    "#Becomes:\n",
    "# 0 0.5 1 | 1\n",
    "\n",
    "'''#modify the data\n",
    "preparedPatients = []\n",
    "for i in range (1,6001):\n",
    "    temp = patients[f\"p{i:06d}\"]\n",
    "    #drop unneccessary fields\n",
    "    temp.drop(['Bilirubin_direct', 'Bilirubin_total', 'TroponinI', 'Fibrinogen', 'Unit1', 'Unit2', 'EtCO2'], axis=1, inplace=True)\n",
    "    row = temp.mean() #using the average per patient if applicatble\n",
    "    row = row.fillna(global_means) #if average can't be calculated per patient, use global average\n",
    "    row['SepsisLabel'] = temp['SepsisLabel'].iloc[-1] #overwrite sepsis label with last label \n",
    "    \n",
    "    preparedPatients.append(row)\n",
    "\n",
    "#store as a dataframe\n",
    "df_patients = pd.DataFrame(preparedPatients)\n",
    "\n",
    "#xy split (not using to_xy)\n",
    "y = df_patients['SepsisLabel'].values.astype(np.float32) \n",
    "x = df_patients.drop('SepsisLabel', axis=1).values.astype(np.float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load In The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering that there are many more patients without sepsis than with sepsis,\n",
    "#it will help balance results to get every instance of sepsis\n",
    "\n",
    "#get every instance of patient with sepsis in both datasets, put in a library\n",
    "patients = {}\n",
    "j = 1\n",
    "\n",
    "#dataset A\n",
    "for i in range (0,20644):\n",
    "    try:\n",
    "        filename = f\"CC-2019-Sepsis/training_setA/p{i:06d}.psv\"\n",
    "        temp = pd.read_csv(filename, sep='|')\n",
    "        #if the patient has sepsis, store in library\n",
    "        if temp['SepsisLabel'].iloc[-1] == 1:\n",
    "            key = f\"p{j:06d}\"\n",
    "            patients[key] = temp\n",
    "            j += 1\n",
    "    except FileNotFoundError as e:\n",
    "        if testing == True:\n",
    "            print(f\"file not found p{i}.psv\")\n",
    "        \n",
    "#dataset B\n",
    "for i in range (100000,120001):\n",
    "    try:\n",
    "        filename = f\"CC-2019-Sepsis/training_setB/p{i:06d}.psv\"\n",
    "        temp = pd.read_csv(filename, sep='|')\n",
    "        #if the patient has sepsis, store in library\n",
    "        if temp['SepsisLabel'].iloc[-1] == 1:\n",
    "            key = f\"p{j:06d}\"\n",
    "            patients[key] = temp\n",
    "            j += 1\n",
    "    except FileNotFoundError as e:\n",
    "        if testing == True:\n",
    "            print(f\"file not found p{i}.psv\")\n",
    "        \n",
    "#number of sepsis patients found        \n",
    "numSepsisPatients = (len(patients))\n",
    "\n",
    "#add in non sepsis patients from dataset A for a total of 6000\n",
    "for i in range (0,20643):\n",
    "    if j > 6000:\n",
    "        break\n",
    "    try:\n",
    "        filename = f\"CC-2019-Sepsis/training_setA/p{i:06d}.psv\"\n",
    "        temp = pd.read_csv(filename, sep='|')\n",
    "        if temp['SepsisLabel'].iloc[-1] == 0:\n",
    "            key = f\"p{j:06d}\"\n",
    "            patients[key] = temp\n",
    "            j += 1\n",
    "    except FileNotFoundError as e:\n",
    "        if testing == True:\n",
    "            print(f\"file not found p{i}.psv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the global average for all data\n",
    "\n",
    "all_dfs = []\n",
    "#dataset A\n",
    "for i in range(0, 20644):\n",
    "    try:\n",
    "        filename = f\"CC-2019-Sepsis/training_setA/p{i:06d}.psv\"\n",
    "        temp = pd.read_csv(filename, sep='|')\n",
    "        all_dfs.append(temp)\n",
    "    except FileNotFoundError:\n",
    "        if testing == True:\n",
    "            print(f\"file not found p{i}.psv\")\n",
    "#dataset B\n",
    "for i in range (100000,120001):\n",
    "    try:\n",
    "        filename = f\"CC-2019-Sepsis/training_setB/p{i:06d}.psv\"\n",
    "        temp = pd.read_csv(filename, sep='|')\n",
    "        all_dfs.append(temp)\n",
    "    except FileNotFoundError as e:\n",
    "        if testing == True:\n",
    "            print(f\"file not found p{i}.psv\")\n",
    "global_df = pd.concat(all_dfs, ignore_index=True)\n",
    "global_means = global_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the calculated means & current number of patients loaded in\n",
    "if testingResults == True:\n",
    "    print(global_means)\n",
    "    print(f\"number of sepsis patients: {numSepsisPatients}\")\n",
    "    print(f\"total number of patients: {len(patients)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkout x and y values for patient 1\n",
    "if testingResults == True:\n",
    "    print(x[1])\n",
    "    print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test script to find the biggest window size we can have without padding (it's 8)\n",
    "if testing == True:\n",
    "    min_rows = float('inf')\n",
    "    min_file = None\n",
    "\n",
    "    for i in range(0, 20644):\n",
    "        try:\n",
    "            filename = f\"CC-2019-Sepsis/training_setA/p{i:06d}.psv\"\n",
    "            temp = pd.read_csv(filename, sep='|')\n",
    "            if len(temp) < min_rows:\n",
    "                min_rows = len(temp)\n",
    "                min_file = f\"p{i:06d}\"\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    print(f\"Smallest file: {min_file} with {min_rows} rows\")\n",
    "    for i in range (100000,120001):\n",
    "        try:\n",
    "            filename = f\"CC-2019-Sepsis/training_setB/p{i:06d}.psv\"\n",
    "            temp = pd.read_csv(filename, sep='|')\n",
    "            if len(temp) < min_rows:\n",
    "                min_rows = len(temp)\n",
    "                min_file = f\"p{i:06d}\"\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    print(f\"Smallest file: {min_file} with {min_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define how many time entries per patient\n",
    "WINDOW_SIZE = 8 \n",
    "#columns to be dropped\n",
    "DROP_COLS = ['Bilirubin_direct', 'Bilirubin_total', 'TroponinI', 'Fibrinogen', 'Unit1', 'Unit2', 'EtCO2', 'SepsisLabel']\n",
    "\n",
    "preparedPatients = []\n",
    "labels = []\n",
    "\n",
    "for i in range(1, 6001):\n",
    "    temp = patients[f\"p{i:06d}\"]\n",
    "    temp = temp.drop(DROP_COLS, axis=1) #drop unneccessary fields and SepsisLabel\n",
    "    temp = temp.fillna(global_means) #fill NaNs with global means\n",
    "    \n",
    "    #pad with global mean values for each column if patient has fewer than WINDOW_SIZE rows\n",
    "    if len(temp) < WINDOW_SIZE:\n",
    "        padding = pd.DataFrame([global_means.drop(DROP_COLS)] * (WINDOW_SIZE - len(temp)))\n",
    "        temp = pd.concat([temp, padding], ignore_index=True)\n",
    "    \n",
    "    #take WINDOW_SIZE rows from end and flatten to 1D\n",
    "    window = temp.iloc[-WINDOW_SIZE:].values.astype(np.float32)\n",
    "    preparedPatients.append(window.flatten())\n",
    "    \n",
    "    #overwrite sepsis label with last label \n",
    "    labels.append(patients[f\"p{i:06d}\"]['SepsisLabel'].iloc[-1])\n",
    "\n",
    "#set x and y values (not using to_xy())\n",
    "x = np.array(preparedPatients, dtype=np.float32)\n",
    "y = np.array(labels, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the shape of x and y values as well as values for first entry of x\n",
    "if testingResults == True:\n",
    "    print(f\"x shape: {x.shape}\\ny shape: {y.shape}\")\n",
    "    print(f\"values for first entry of x: {x[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split, 80/20 split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contrast original dataframe sizes with split dataframe sizes\n",
    "if testingResults == True:\n",
    "    print(f\"x_test shape: {x_test.shape}\\nx_train shape: {x_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\\ny_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes\n",
    "-The data is now cleaned, split into x/y and then split into train/test for both x and y<br><br>\n",
    "-The approach is to take out a window size from each file (first 8 entries) to use as x, filling in NaN values with either the local or global mean<br><br>\n",
    "-The y value is the final SepsisLabel in the dataset, as that value will tell us if the patient had sepsis or not<br><br>\n",
    "-This approach lines up very well with EC 4, as the window size functionality is already implemented.  Padding has also been implemented if necessary to test window sizes greater than 8 (the smallest files only have 8 time entries)<br><br>\n",
    "-The data chosen specifically brings in ALL instances of sepsis patients from both datasets (roughly 2900) and pairs it with a similar amount of non sepsis patients (roughly 3100).  This technically could qualify as undersampling the non-sepsis data, which would fulfull EC 1 if we wanted to contrast it with an additional method of reading in the data that just takes the first 6000 entries instead of scanning for specific entries.  That being said, if there was some other EC you would rather do instead let me know<br><br>\n",
    "-Ensure you put the unzipped data file CC-2019-Sepsis in the same directory as CSC296SProj1.ipynb<br><br>\n",
    "-At this stage, assuming there are no bugs the data should be ready to be put into some models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
